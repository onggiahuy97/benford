{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2b02592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (58596, 57)\n",
      "Columns: ['Category', 'pslist.nproc', 'pslist.nppid', 'pslist.avg_threads', 'pslist.nprocs64bit', 'pslist.avg_handlers', 'dlllist.ndlls', 'dlllist.avg_dlls_per_proc', 'handles.nhandles', 'handles.avg_handles_per_proc', 'handles.nport', 'handles.nfile', 'handles.nevent', 'handles.ndesktop', 'handles.nkey', 'handles.nthread', 'handles.ndirectory', 'handles.nsemaphore', 'handles.ntimer', 'handles.nsection', 'handles.nmutant', 'ldrmodules.not_in_load', 'ldrmodules.not_in_init', 'ldrmodules.not_in_mem', 'ldrmodules.not_in_load_avg', 'ldrmodules.not_in_init_avg', 'ldrmodules.not_in_mem_avg', 'malfind.ninjections', 'malfind.commitCharge', 'malfind.protection', 'malfind.uniqueInjections', 'psxview.not_in_pslist', 'psxview.not_in_eprocess_pool', 'psxview.not_in_ethread_pool', 'psxview.not_in_pspcid_list', 'psxview.not_in_csrss_handles', 'psxview.not_in_session', 'psxview.not_in_deskthrd', 'psxview.not_in_pslist_false_avg', 'psxview.not_in_eprocess_pool_false_avg', 'psxview.not_in_ethread_pool_false_avg', 'psxview.not_in_pspcid_list_false_avg', 'psxview.not_in_csrss_handles_false_avg', 'psxview.not_in_session_false_avg', 'psxview.not_in_deskthrd_false_avg', 'modules.nmodules', 'svcscan.nservices', 'svcscan.kernel_drivers', 'svcscan.fs_drivers', 'svcscan.process_services', 'svcscan.shared_process_services', 'svcscan.interactive_process_services', 'svcscan.nactive', 'callbacks.ncallbacks', 'callbacks.nanonymous', 'callbacks.ngeneric', 'Class']\n",
      "\n",
      "Dataset Info:\n",
      "Total samples: 58596\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_cicmalmem_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Load the CICMalMem 2022 dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Display basic info\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(f\"Total samples: {len(df)}\")\n",
    "        if 'label' in df.columns:\n",
    "            print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "        elif 'Label' in df.columns:\n",
    "            print(f\"Label distribution:\\n{df['Label'].value_counts()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'path_to_your_dataset.csv' with actual path\n",
    "df = load_cicmalmem_dataset('Obfuscated-MalMem2022.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9df734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_digit(value):\n",
    "    \"\"\"\n",
    "    Extract the first significant digit from a number\n",
    "    Following the paper's methodology (Equation 3)\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return 0\n",
    "    \n",
    "    # Convert to numeric if it's a string\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            # Try to convert string to float\n",
    "            value = float(value)\n",
    "        \n",
    "        if value == 0:\n",
    "            return 0\n",
    "            \n",
    "    except (ValueError, TypeError):\n",
    "        # If conversion fails, return 0\n",
    "        return 0\n",
    "    \n",
    "    # Take absolute value (modulus operation from paper)\n",
    "    abs_value = abs(float(value))\n",
    "    \n",
    "    # Collapse the number to extract first digit\n",
    "    # D_collapsed = |10 * a / 10^int(log10(a))|\n",
    "    if abs_value < 1:\n",
    "        # For numbers < 1, multiply until >= 1\n",
    "        while abs_value < 1 and abs_value > 0:\n",
    "            abs_value *= 10\n",
    "    \n",
    "    if abs_value == 0:\n",
    "        return 0\n",
    "        \n",
    "    # Extract first digit\n",
    "    try:\n",
    "        first_digit = int(str(abs_value)[0])\n",
    "        return first_digit if first_digit != 0 else 1\n",
    "    except (ValueError, IndexError):\n",
    "        return 0\n",
    "\n",
    "def get_digit_frequencies(data_series):\n",
    "    \"\"\"\n",
    "    Calculate the frequency of each digit (1-9) in a data series\n",
    "    \"\"\"\n",
    "    # Convert series to list and ensure numeric values\n",
    "    if isinstance(data_series, pd.Series):\n",
    "        data_list = data_series.tolist()\n",
    "    else:\n",
    "        data_list = list(data_series)\n",
    "    \n",
    "    # Extract first digits with improved error handling\n",
    "    first_digits = []\n",
    "    for val in data_list:\n",
    "        digit = extract_first_digit(val)\n",
    "        if digit > 0:  # Only include valid digits 1-9\n",
    "            first_digits.append(digit)\n",
    "    \n",
    "    # Count frequencies\n",
    "    digit_counts = {i: first_digits.count(i) for i in range(1, 10)}\n",
    "    total_count = sum(digit_counts.values())\n",
    "    \n",
    "    if total_count == 0:\n",
    "        return np.zeros(9)\n",
    "    \n",
    "    # Calculate relative frequencies\n",
    "    frequencies = np.array([digit_counts[i] / total_count for i in range(1, 10)])\n",
    "    return frequencies\n",
    "\n",
    "def clean_and_prepare_data(df, label_col='binary_label'):\n",
    "    \"\"\"\n",
    "    Clean and prepare the dataset for Benford's Law analysis\n",
    "    \"\"\"\n",
    "    print(\"Cleaning and preparing data...\")\n",
    "    \n",
    "    # Create a copy\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert label to binary if needed\n",
    "    if label_col not in df_clean.columns:\n",
    "        if 'Class' in df_clean.columns:\n",
    "            unique_classes = df_clean['Class'].unique()\n",
    "            print(f\"Unique classes: {unique_classes}\")\n",
    "            \n",
    "            # Map to binary (assuming first class is benign=0, others malicious=1)\n",
    "            if len(unique_classes) == 2:\n",
    "                df_clean['binary_label'] = (df_clean['Class'] != unique_classes[0]).astype(int)\n",
    "                label_col = 'binary_label'\n",
    "            else:\n",
    "                # For multi-class, convert to binary (benign vs malicious)\n",
    "                benign_classes = ['Benign', 'benign', 'BENIGN', 'Normal', 'normal']\n",
    "                df_clean['binary_label'] = (~df_clean['Class'].isin(benign_classes)).astype(int)\n",
    "                label_col = 'binary_label'\n",
    "    \n",
    "    # Get all potential feature columns (exclude categorical columns)\n",
    "    feature_cols = []\n",
    "    exclude_cols = ['Category', 'Class', 'binary_label']\n",
    "    \n",
    "    for col in df_clean.columns:\n",
    "        if col not in exclude_cols:\n",
    "            feature_cols.append(col)\n",
    "    \n",
    "    print(f\"Found {len(feature_cols)} potential feature columns\")\n",
    "    \n",
    "    # Convert all feature columns to numeric\n",
    "    for col in feature_cols:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Remove columns with too many NaN values (>50%)\n",
    "    valid_cols = []\n",
    "    for col in feature_cols:\n",
    "        nan_ratio = df_clean[col].isna().sum() / len(df_clean)\n",
    "        if nan_ratio < 0.5:  # Keep columns with <50% NaN values\n",
    "            valid_cols.append(col)\n",
    "        else:\n",
    "            print(f\"Removing {col}: {nan_ratio:.2%} NaN values\")\n",
    "    \n",
    "    print(f\"Kept {len(valid_cols)} valid feature columns\")\n",
    "    \n",
    "    # Fill remaining NaN values with 0 (common approach for this type of data)\n",
    "    for col in valid_cols:\n",
    "        df_clean[col] = df_clean[col].fillna(0)\n",
    "    \n",
    "    return df_clean, valid_cols, label_col\n",
    "\n",
    "# Updated feature selection function\n",
    "def select_features_benford_improved(df, feature_cols, label_column='binary_label'):\n",
    "    \"\"\"\n",
    "    Improved feature selection that handles data type issues\n",
    "    \"\"\"\n",
    "    # Define thresholds from the paper\n",
    "    PEARSON_THRESHOLD = 0.5  # Lowered for initial testing\n",
    "    KL_THRESHOLD = 0.20      # Increased for initial testing\n",
    "    EUCLIDEAN_THRESHOLD = 0.30\n",
    "    MAD_THRESHOLD = 0.01     # Increased for initial testing\n",
    "    \n",
    "    print(f\"Analyzing {len(feature_cols)} numerical features...\")\n",
    "    \n",
    "    # Calculate Benford's distribution\n",
    "    _, benford_expected = calculate_benford_distribution()\n",
    "    \n",
    "    # Store results\n",
    "    feature_analysis = []\n",
    "    dist_func = DistanceFunctions()\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        try:\n",
    "            # Get data for this feature\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            # Skip if not enough data or all zeros\n",
    "            if len(col_data) < 100 or col_data.nunique() < 5:\n",
    "                continue\n",
    "            \n",
    "            # Get observed frequencies for this feature\n",
    "            observed_freq = get_digit_frequencies(col_data)\n",
    "            \n",
    "            # Skip if not enough digit variety\n",
    "            if np.sum(observed_freq) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate all distance metrics\n",
    "            kl_div = dist_func.kullback_leibler_divergence(observed_freq, benford_expected)\n",
    "            js_div = dist_func.jensen_shannon_divergence(observed_freq, benford_expected)\n",
    "            mad = dist_func.mean_absolute_deviation(observed_freq, benford_expected)\n",
    "            pearson_corr = dist_func.pearson_correlation(observed_freq, benford_expected)\n",
    "            euclidean_dist = dist_func.euclidean_distance(observed_freq, benford_expected)\n",
    "            ks_stat, ks_pvalue = dist_func.kolmogorov_smirnov_test(observed_freq, benford_expected)\n",
    "            \n",
    "            # Count how many criteria are met\n",
    "            criteria_met = 0\n",
    "            if not np.isnan(pearson_corr) and pearson_corr >= PEARSON_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            if not np.isnan(kl_div) and kl_div <= KL_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            if not np.isnan(euclidean_dist) and euclidean_dist <= EUCLIDEAN_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            if not np.isnan(mad) and mad <= MAD_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            \n",
    "            feature_analysis.append({\n",
    "                'feature': col,\n",
    "                'kl_divergence': kl_div,\n",
    "                'js_divergence': js_div,\n",
    "                'mad': mad,\n",
    "                'pearson_correlation': pearson_corr,\n",
    "                'euclidean_distance': euclidean_dist,\n",
    "                'ks_statistic': ks_stat,\n",
    "                'ks_pvalue': ks_pvalue,\n",
    "                'criteria_met': criteria_met,\n",
    "                'benford_conformity': pearson_corr if not np.isnan(pearson_corr) else 0\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing feature {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(feature_analysis)\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"No valid features found! Using top features by data quality...\")\n",
    "        # Fallback: select features with good data quality\n",
    "        selected_features = feature_cols[:10]  # Take first 10 as fallback\n",
    "    else:\n",
    "        # Select features that meet at least 1 criteria (relaxed from 2)\n",
    "        candidate_features = results_df[results_df['criteria_met'] >= 1]\n",
    "        \n",
    "        if len(candidate_features) == 0:\n",
    "            # Further fallback: select top features by Pearson correlation\n",
    "            candidate_features = results_df.nlargest(10, 'benford_conformity')\n",
    "        \n",
    "        selected_features = candidate_features['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nFeature Analysis Results:\")\n",
    "    print(f\"Total features analyzed: {len(results_df)}\")\n",
    "    print(f\"Features meeting ≥1 criteria: {len(selected_features)}\")\n",
    "    \n",
    "    # Show top features\n",
    "    if len(results_df) > 0:\n",
    "        top_features = results_df.nlargest(10, 'benford_conformity')\n",
    "        print(f\"\\nTop 10 Features by Benford's Law Conformity:\")\n",
    "        for _, row in top_features.iterrows():\n",
    "            print(f\"{row['feature']}: Pearson={row['pearson_correlation']:.3f}, \"\n",
    "                  f\"KL={row['kl_divergence']:.3f}, Criteria={row['criteria_met']}\")\n",
    "    \n",
    "    return selected_features, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ab777b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENFORD'S LAW MALWARE DETECTION - COMPLETE ANALYSIS\n",
      "======================================================================\n",
      "Cleaning and preparing data...\n",
      "Found 55 potential feature columns\n",
      "Kept 55 valid feature columns\n",
      "\n",
      "Label distribution after cleaning:\n",
      "binary_label\n",
      "0    29298\n",
      "1    29298\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "FEATURE SELECTION BASED ON BENFORD'S LAW\n",
      "==================================================\n",
      "Analyzing 55 numerical features...\n",
      "\n",
      "Feature Analysis Results:\n",
      "Total features analyzed: 48\n",
      "Features meeting ≥1 criteria: 23\n",
      "\n",
      "Top 10 Features by Benford's Law Conformity:\n",
      "psxview.not_in_ethread_pool: Pearson=0.994, KL=0.036, Criteria=3\n",
      "psxview.not_in_pspcid_list: Pearson=0.990, KL=0.031, Criteria=3\n",
      "psxview.not_in_pslist: Pearson=0.990, KL=0.031, Criteria=3\n",
      "malfind.protection: Pearson=0.968, KL=0.249, Criteria=2\n",
      "psxview.not_in_deskthrd_false_avg: Pearson=0.965, KL=0.585, Criteria=1\n",
      "dlllist.ndlls: Pearson=0.946, KL=0.690, Criteria=1\n",
      "pslist.nppid: Pearson=0.870, KL=1.061, Criteria=1\n",
      "malfind.uniqueInjections: Pearson=0.869, KL=1.089, Criteria=1\n",
      "svcscan.nactive: Pearson=0.864, KL=1.196, Criteria=1\n",
      "svcscan.shared_process_services: Pearson=0.864, KL=1.195, Criteria=1\n",
      "\n",
      "Selected 23 features for modeling:\n",
      "1. pslist.nppid\n",
      "2. pslist.avg_threads\n",
      "3. dlllist.ndlls\n",
      "4. handles.nhandles\n",
      "5. handles.nthread\n",
      "6. handles.ndirectory\n",
      "7. handles.ntimer\n",
      "8. handles.nsection\n",
      "9. ldrmodules.not_in_init\n",
      "10. malfind.protection\n",
      "\n",
      "==================================================\n",
      "MODEL TRAINING AND EVALUATION\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING AND EVALUATING BENFORD'S LAW MODEL\n",
      "============================================================\n",
      "Using 124 features for training\n",
      "Dataset size: 58596 samples\n",
      "Label distribution: [29298 29298]\n",
      "Train set: 46876 samples\n",
      "Test set: 11720 samples\n",
      "Training on 23 selected features...\n",
      "Model trained on 23 features\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "❌ Error during analysis: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/065496ks5dxfngyxnk48wjpr0000gn/T/ipykernel_40349/2219921756.py\", line 43, in <module>\n",
      "    model, results, test_data, selected_features, analysis_results = run_complete_benford_analysis(df)\n",
      "                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/pg/065496ks5dxfngyxnk48wjpr0000gn/T/ipykernel_40349/2219921756.py\", line 37, in run_complete_benford_analysis\n",
      "    model, results, test_data = train_and_evaluate_benford_model(df_clean, selected_features, label_col)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/pg/065496ks5dxfngyxnk48wjpr0000gn/T/ipykernel_40349/4265649076.py\", line 72, in train_and_evaluate_benford_model\n",
      "    y_pred_train = model.predict(X_train)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/pg/065496ks5dxfngyxnk48wjpr0000gn/T/ipykernel_40349/613079078.py\", line 58, in predict\n",
      "    feature_data = [row[feature]] if not pd.isna(row[feature]) else []\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huyong97/Library/Python/3.12/lib/python/site-packages/pandas/core/generic.py\", line 1577, in __nonzero__\n",
      "    raise ValueError(\n",
      "ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"
     ]
    }
   ],
   "source": [
    "def run_complete_benford_analysis(df):\n",
    "    \"\"\"\n",
    "    Run the complete Benford's Law malware detection analysis\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"BENFORD'S LAW MALWARE DETECTION - COMPLETE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Clean and prepare data\n",
    "    df_clean, feature_cols, label_col = clean_and_prepare_data(df)\n",
    "    \n",
    "    print(f\"\\nLabel distribution after cleaning:\")\n",
    "    print(df_clean[label_col].value_counts())\n",
    "    \n",
    "    # Step 2: Feature selection\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FEATURE SELECTION BASED ON BENFORD'S LAW\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    selected_features, analysis_results = select_features_benford_improved(df_clean, feature_cols, label_col)\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        print(\"No features selected! Using top features by variance...\")\n",
    "        # Fallback selection\n",
    "        feature_vars = df_clean[feature_cols].var().sort_values(ascending=False)\n",
    "        selected_features = feature_vars.head(5).index.tolist()\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features for modeling:\")\n",
    "    for i, feature in enumerate(selected_features[:10], 1):  # Show max 10\n",
    "        print(f\"{i}. {feature}\")\n",
    "    \n",
    "    # Step 3: Train and evaluate model\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"MODEL TRAINING AND EVALUATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model, results, test_data = train_and_evaluate_benford_model(df_clean, selected_features, label_col)\n",
    "    \n",
    "    return model, results, test_data, selected_features, analysis_results\n",
    "\n",
    "# Run the complete analysis\n",
    "try:\n",
    "    model, results, test_data, selected_features, analysis_results = run_complete_benford_analysis(df)\n",
    "    print(\"\\n🎉 Analysis completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37704466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Distance Functions:\n",
      "KL Divergence: 0.000001\n",
      "JS Divergence: 0.000000\n",
      "MAD: 0.000101\n",
      "Pearson Correlation: 0.999999\n",
      "Euclidean Distance: 0.000373\n"
     ]
    }
   ],
   "source": [
    "class DistanceFunctions:\n",
    "    \"\"\"\n",
    "    Implementation of all distance functions used in the paper\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def kullback_leibler_divergence(observed, expected):\n",
    "        \"\"\"\n",
    "        Calculate KL divergence: D_KL(P||Q) = Σ P(i) * log(P(i)/Q(i))\n",
    "        \"\"\"\n",
    "        # Add small epsilon to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        observed = observed + epsilon\n",
    "        expected = expected + epsilon\n",
    "        \n",
    "        # Normalize to ensure they sum to 1\n",
    "        observed = observed / np.sum(observed)\n",
    "        expected = expected / np.sum(expected)\n",
    "        \n",
    "        kl_div = np.sum(observed * np.log(observed / expected))\n",
    "        return kl_div\n",
    "    \n",
    "    @staticmethod\n",
    "    def jensen_shannon_divergence(observed, expected):\n",
    "        \"\"\"\n",
    "        Calculate Jensen-Shannon divergence\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10\n",
    "        observed = observed + epsilon\n",
    "        expected = expected + epsilon\n",
    "        \n",
    "        # Normalize\n",
    "        P = observed / np.sum(observed)\n",
    "        Q = expected / np.sum(expected)\n",
    "        \n",
    "        # Calculate M = 0.5 * (P + Q)\n",
    "        M = 0.5 * (P + Q)\n",
    "        \n",
    "        # JS divergence\n",
    "        js_div = 0.5 * np.sum(P * np.log(P / M)) + 0.5 * np.sum(Q * np.log(Q / M))\n",
    "        return js_div\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_absolute_deviation(observed, expected):\n",
    "        \"\"\"\n",
    "        Calculate Mean Absolute Deviation (MAD)\n",
    "        MAD = Σ|Fr - Ef| / N\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs(observed - expected))\n",
    "    \n",
    "    @staticmethod\n",
    "    def pearson_correlation(observed, expected):\n",
    "        \"\"\"\n",
    "        Calculate Pearson correlation coefficient\n",
    "        \"\"\"\n",
    "        correlation, _ = stats.pearsonr(observed, expected)\n",
    "        return correlation if not np.isnan(correlation) else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def kolmogorov_smirnov_test(observed, expected):\n",
    "        \"\"\"\n",
    "        Perform Kolmogorov-Smirnov test\n",
    "        \"\"\"\n",
    "        # Convert to cumulative distributions\n",
    "        observed_cumsum = np.cumsum(observed)\n",
    "        expected_cumsum = np.cumsum(expected)\n",
    "        \n",
    "        # KS statistic is the maximum difference\n",
    "        ks_statistic = np.max(np.abs(observed_cumsum - expected_cumsum))\n",
    "        \n",
    "        # Calculate p-value using scipy\n",
    "        _, p_value = stats.ks_2samp(observed, expected)\n",
    "        \n",
    "        return ks_statistic, p_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def z_statistic(observed, expected, n):\n",
    "        \"\"\"\n",
    "        Calculate Z-statistic for each digit\n",
    "        Z = |AP - FE| - (1/2N) / sqrt(FE * (1-FE) / N)\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return np.zeros_like(observed)\n",
    "        \n",
    "        z_scores = []\n",
    "        for i in range(len(observed)):\n",
    "            numerator = abs(observed[i] - expected[i]) - (1.0 / (2 * n))\n",
    "            denominator = np.sqrt((expected[i] * (1 - expected[i])) / n)\n",
    "            \n",
    "            if denominator == 0:\n",
    "                z_scores.append(0)\n",
    "            else:\n",
    "                z_scores.append(numerator / denominator)\n",
    "        \n",
    "        return np.array(z_scores)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(observed, expected):\n",
    "        \"\"\"\n",
    "        Calculate Euclidean distance between observed and expected frequencies\n",
    "        \"\"\"\n",
    "        return euclidean(observed, expected)\n",
    "\n",
    "# Test the distance functions\n",
    "print(\"Testing Distance Functions:\")\n",
    "test_observed = np.array([0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046])\n",
    "test_expected = benford_prob\n",
    "\n",
    "dist_func = DistanceFunctions()\n",
    "print(f\"KL Divergence: {dist_func.kullback_leibler_divergence(test_observed, test_expected):.6f}\")\n",
    "print(f\"JS Divergence: {dist_func.jensen_shannon_divergence(test_observed, test_expected):.6f}\")\n",
    "print(f\"MAD: {dist_func.mean_absolute_deviation(test_observed, test_expected):.6f}\")\n",
    "print(f\"Pearson Correlation: {dist_func.pearson_correlation(test_observed, test_expected):.6f}\")\n",
    "print(f\"Euclidean Distance: {dist_func.euclidean_distance(test_observed, test_expected):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c76428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 56 numerical features...\n",
      "\n",
      "Feature Analysis Results:\n",
      "Total features analyzed: 53\n",
      "Features meeting ≥2 criteria: 3\n",
      "\n",
      "Top 10 Features by Benford's Law Conformity:\n",
      "psxview.not_in_ethread_pool: Pearson=0.994, KL=0.036, Criteria=3\n",
      "psxview.not_in_pspcid_list: Pearson=0.990, KL=0.031, Criteria=3\n",
      "psxview.not_in_pslist: Pearson=0.990, KL=0.031, Criteria=3\n",
      "malfind.protection: Pearson=0.968, KL=0.249, Criteria=1\n",
      "psxview.not_in_deskthrd_false_avg: Pearson=0.965, KL=0.585, Criteria=1\n",
      "dlllist.ndlls: Pearson=0.946, KL=0.690, Criteria=1\n",
      "pslist.nppid: Pearson=0.870, KL=1.061, Criteria=1\n",
      "malfind.uniqueInjections: Pearson=0.869, KL=1.089, Criteria=1\n",
      "psxview.not_in_eprocess_pool: Pearson=0.864, KL=1.201, Criteria=1\n",
      "modules.nmodules: Pearson=0.864, KL=1.201, Criteria=1\n"
     ]
    }
   ],
   "source": [
    "def select_features_benford(df, label_column='label'):\n",
    "    \"\"\"\n",
    "    Select features that best conform to Benford's Law\n",
    "    Based on the paper's methodology and thresholds\n",
    "    \"\"\"\n",
    "    # Define thresholds from the paper\n",
    "    PEARSON_THRESHOLD = 0.85\n",
    "    KL_THRESHOLD = 0.10\n",
    "    EUCLIDEAN_THRESHOLD = 0.20\n",
    "    MAD_THRESHOLD = 1e-5\n",
    "    \n",
    "    # Get numerical columns (exclude label)\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if label_column in numerical_cols:\n",
    "        numerical_cols.remove(label_column)\n",
    "    \n",
    "    print(f\"Analyzing {len(numerical_cols)} numerical features...\")\n",
    "    \n",
    "    # Calculate Benford's distribution\n",
    "    _, benford_expected = calculate_benford_distribution()\n",
    "    \n",
    "    # Store results\n",
    "    feature_analysis = []\n",
    "    dist_func = DistanceFunctions()\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        try:\n",
    "            # Get observed frequencies for this feature\n",
    "            observed_freq = get_digit_frequencies(df[col].dropna())\n",
    "            \n",
    "            # Skip if not enough data\n",
    "            if np.sum(observed_freq) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate all distance metrics\n",
    "            kl_div = dist_func.kullback_leibler_divergence(observed_freq, benford_expected)\n",
    "            js_div = dist_func.jensen_shannon_divergence(observed_freq, benford_expected)\n",
    "            mad = dist_func.mean_absolute_deviation(observed_freq, benford_expected)\n",
    "            pearson_corr = dist_func.pearson_correlation(observed_freq, benford_expected)\n",
    "            euclidean_dist = dist_func.euclidean_distance(observed_freq, benford_expected)\n",
    "            ks_stat, ks_pvalue = dist_func.kolmogorov_smirnov_test(observed_freq, benford_expected)\n",
    "            \n",
    "            # Count how many criteria are met (paper's approach)\n",
    "            criteria_met = 0\n",
    "            if pearson_corr >= PEARSON_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            if kl_div <= KL_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            if euclidean_dist <= EUCLIDEAN_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            if mad <= MAD_THRESHOLD:\n",
    "                criteria_met += 1\n",
    "            \n",
    "            feature_analysis.append({\n",
    "                'feature': col,\n",
    "                'kl_divergence': kl_div,\n",
    "                'js_divergence': js_div,\n",
    "                'mad': mad,\n",
    "                'pearson_correlation': pearson_corr,\n",
    "                'euclidean_distance': euclidean_dist,\n",
    "                'ks_statistic': ks_stat,\n",
    "                'ks_pvalue': ks_pvalue,\n",
    "                'criteria_met': criteria_met,\n",
    "                'benford_conformity': pearson_corr if not np.isnan(pearson_corr) else 0\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing feature {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(feature_analysis)\n",
    "    \n",
    "    # Select features that meet at least 2 criteria (paper's approach)\n",
    "    selected_features = results_df[results_df['criteria_met'] >= 2]['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nFeature Analysis Results:\")\n",
    "    print(f\"Total features analyzed: {len(results_df)}\")\n",
    "    print(f\"Features meeting ≥2 criteria: {len(selected_features)}\")\n",
    "    \n",
    "    # Show top features by Benford conformity\n",
    "    top_features = results_df.nlargest(10, 'benford_conformity')\n",
    "    print(f\"\\nTop 10 Features by Benford's Law Conformity:\")\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"{row['feature']}: Pearson={row['pearson_correlation']:.3f}, \"\n",
    "              f\"KL={row['kl_divergence']:.3f}, Criteria={row['criteria_met']}\")\n",
    "    \n",
    "    return selected_features, results_df\n",
    "\n",
    "# Apply feature selection (you'll run this once you have your dataset loaded)\n",
    "selected_features, analysis_results = select_features_benford(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c295350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking label distributions:\n",
      "\n",
      "Category column:\n",
      "Category\n",
      "Benign                                                                                     29298\n",
      "Spyware-Gator-1bdcd3b777965f67678748d2577b119a275aca9aed9549d45e64e692a54a7b5e-1.raw           2\n",
      "Spyware-Gator-1bfb316482877ee42e5a5078fef44c0eb51bc44c1e88ecbccd02ce4dc4694bd3-2.raw           2\n",
      "Spyware-Gator-1bfb316482877ee42e5a5078fef44c0eb51bc44c1e88ecbccd02ce4dc4694bd3-10.raw          2\n",
      "Spyware-Gator-1bfb316482877ee42e5a5078fef44c0eb51bc44c1e88ecbccd02ce4dc4694bd3-1.raw           2\n",
      "                                                                                           ...  \n",
      "Spyware-Gator-0b25829d15dc951a44e7652fc6de9d936d7d51f29586d56dbf8fccea419252ac-6.raw           1\n",
      "Spyware-Gator-0b25829d15dc951a44e7652fc6de9d936d7d51f29586d56dbf8fccea419252ac-5.raw           1\n",
      "Spyware-Gator-0b25829d15dc951a44e7652fc6de9d936d7d51f29586d56dbf8fccea419252ac-4.raw           1\n",
      "Spyware-Gator-0b25829d15dc951a44e7652fc6de9d936d7d51f29586d56dbf8fccea419252ac-3.raw           1\n",
      "Ransomware-Shade-955d9af38346c1755527bd196668edfad6d3f001d217b04d2380eb99e0760585-8.raw        1\n",
      "Name: count, Length: 28346, dtype: int64\n",
      "\n",
      "Class column:\n",
      "Class\n",
      "Benign     29298\n",
      "Malware    29298\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique labels in Class: ['Benign' 'Malware']\n",
      "\n",
      "Final label distribution:\n",
      "binary_label\n",
      "0    29298\n",
      "1    29298\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "RUNNING BENFORD'S LAW FEATURE SELECTION\n",
      "============================================================\n",
      "Analyzing 55 numerical features...\n",
      "\n",
      "Feature Analysis Results:\n",
      "Total features analyzed: 52\n",
      "Features meeting ≥2 criteria: 3\n",
      "\n",
      "Top 10 Features by Benford's Law Conformity:\n",
      "psxview.not_in_ethread_pool: Pearson=0.994, KL=0.036, Criteria=3\n",
      "psxview.not_in_pspcid_list: Pearson=0.990, KL=0.031, Criteria=3\n",
      "psxview.not_in_pslist: Pearson=0.990, KL=0.031, Criteria=3\n",
      "malfind.protection: Pearson=0.968, KL=0.249, Criteria=1\n",
      "psxview.not_in_deskthrd_false_avg: Pearson=0.965, KL=0.585, Criteria=1\n",
      "dlllist.ndlls: Pearson=0.946, KL=0.690, Criteria=1\n",
      "pslist.nppid: Pearson=0.870, KL=1.061, Criteria=1\n",
      "malfind.uniqueInjections: Pearson=0.869, KL=1.089, Criteria=1\n",
      "psxview.not_in_eprocess_pool: Pearson=0.864, KL=1.201, Criteria=1\n",
      "modules.nmodules: Pearson=0.864, KL=1.201, Criteria=1\n"
     ]
    }
   ],
   "source": [
    "# Check the label columns\n",
    "print(\"Checking label distributions:\")\n",
    "print(\"\\nCategory column:\")\n",
    "print(df['Category'].value_counts())\n",
    "print(\"\\nClass column:\")\n",
    "print(df['Class'].value_counts())\n",
    "\n",
    "# Let's use 'Class' as our main label (looks like it's binary)\n",
    "# Prepare labels - convert to binary if needed\n",
    "if df['Class'].dtype == 'object':\n",
    "    # Convert string labels to binary\n",
    "    unique_labels = df['Class'].unique()\n",
    "    print(f\"\\nUnique labels in Class: {unique_labels}\")\n",
    "    \n",
    "    # Map to binary (0=benign, 1=malicious)\n",
    "    label_mapping = {}\n",
    "    if len(unique_labels) == 2:\n",
    "        # Assume first unique value is benign (0), second is malicious (1)\n",
    "        label_mapping = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "        df['binary_label'] = df['Class'].map(label_mapping)\n",
    "    else:\n",
    "        # If more than 2 classes, we might need to group them\n",
    "        print(\"Multiple classes detected. Please specify which should be considered malicious.\")\n",
    "\n",
    "print(f\"\\nFinal label distribution:\")\n",
    "if 'binary_label' in df.columns:\n",
    "    print(df['binary_label'].value_counts())\n",
    "    label_col = 'binary_label'\n",
    "else:\n",
    "    label_col = 'Class'\n",
    "\n",
    "# Now run feature selection\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING BENFORD'S LAW FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "selected_features, analysis_results = select_features_benford(df, label_column=label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac8382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenfordMalwareDetector:\n",
    "    \"\"\"\n",
    "    Benford's Law-based Malware Detection Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.selected_features = []\n",
    "        self.benford_expected = None\n",
    "        self.feature_thresholds = {}\n",
    "        self.dist_func = DistanceFunctions()\n",
    "        \n",
    "    def fit(self, X, y, selected_features):\n",
    "        \"\"\"\n",
    "        Fit the model by calculating thresholds for each selected feature\n",
    "        \"\"\"\n",
    "        self.selected_features = selected_features\n",
    "        _, self.benford_expected = calculate_benford_distribution()\n",
    "        \n",
    "        print(f\"Training on {len(selected_features)} selected features...\")\n",
    "        \n",
    "        # Calculate thresholds for each feature based on benign samples\n",
    "        benign_mask = (y == 0)\n",
    "        \n",
    "        for feature in selected_features:\n",
    "            if feature in X.columns:\n",
    "                # Get benign samples for this feature\n",
    "                benign_data = X[benign_mask][feature].dropna()\n",
    "                \n",
    "                if len(benign_data) > 0:\n",
    "                    # Calculate observed frequencies for benign data\n",
    "                    observed_freq = get_digit_frequencies(benign_data)\n",
    "                    \n",
    "                    # Calculate distance metrics\n",
    "                    kl_div = self.dist_func.kullback_leibler_divergence(observed_freq, self.benford_expected)\n",
    "                    pearson_corr = self.dist_func.pearson_correlation(observed_freq, self.benford_expected)\n",
    "                    \n",
    "                    # Store thresholds (use benign behavior as baseline)\n",
    "                    self.feature_thresholds[feature] = {\n",
    "                        'kl_threshold': kl_div * 1.5,  # Allow some deviation\n",
    "                        'pearson_threshold': max(0.5, pearson_corr * 0.8)  # Minimum correlation\n",
    "                    }\n",
    "        \n",
    "        print(f\"Model trained on {len(self.feature_thresholds)} features\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the best combination from the paper: KL + Pearson correlation\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for idx, row in X.iterrows():\n",
    "            suspicion_score = 0\n",
    "            valid_features = 0\n",
    "            \n",
    "            for feature in self.selected_features:\n",
    "                if feature in X.columns:\n",
    "                    feature_data = [row[feature]] if not pd.isna(row[feature]) else []\n",
    "                    \n",
    "                    if len(feature_data) > 0:\n",
    "                        # Get observed frequencies for this sample's feature\n",
    "                        observed_freq = get_digit_frequencies(feature_data * 100)  # Replicate to get frequencies\n",
    "                        \n",
    "                        if np.sum(observed_freq) > 0:\n",
    "                            # Calculate KL divergence and Pearson correlation\n",
    "                            kl_div = self.dist_func.kullback_leibler_divergence(observed_freq, self.benford_expected)\n",
    "                            pearson_corr = self.dist_func.pearson_correlation(observed_freq, self.benford_expected)\n",
    "                            \n",
    "                            # Check against thresholds\n",
    "                            if feature in self.feature_thresholds:\n",
    "                                thresholds = self.feature_thresholds[feature]\n",
    "                                \n",
    "                                # High KL divergence = suspicious\n",
    "                                if kl_div > thresholds['kl_threshold']:\n",
    "                                    suspicion_score += 1\n",
    "                                \n",
    "                                # Low correlation = suspicious  \n",
    "                                if pearson_corr < thresholds['pearson_threshold']:\n",
    "                                    suspicion_score += 1\n",
    "                                \n",
    "                                valid_features += 1\n",
    "            \n",
    "            # Make prediction based on suspicion score\n",
    "            if valid_features > 0:\n",
    "                suspicion_ratio = suspicion_score / (valid_features * 2)  # *2 because we have 2 criteria per feature\n",
    "                prediction = 1 if suspicion_ratio > 0.5 else 0  # Threshold from paper's methodology\n",
    "            else:\n",
    "                prediction = 0  # Default to benign if no valid features\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return prediction probabilities\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        \n",
    "        for idx, row in X.iterrows():\n",
    "            suspicion_score = 0\n",
    "            valid_features = 0\n",
    "            \n",
    "            for feature in self.selected_features:\n",
    "                if feature in X.columns:\n",
    "                    feature_data = [row[feature]] if not pd.isna(row[feature]) else []\n",
    "                    \n",
    "                    if len(feature_data) > 0:\n",
    "                        observed_freq = get_digit_frequencies(feature_data * 100)\n",
    "                        \n",
    "                        if np.sum(observed_freq) > 0:\n",
    "                            kl_div = self.dist_func.kullback_leibler_divergence(observed_freq, self.benford_expected)\n",
    "                            pearson_corr = self.dist_func.pearson_correlation(observed_freq, self.benford_expected)\n",
    "                            \n",
    "                            if feature in self.feature_thresholds:\n",
    "                                thresholds = self.feature_thresholds[feature]\n",
    "                                \n",
    "                                if kl_div > thresholds['kl_threshold']:\n",
    "                                    suspicion_score += 1\n",
    "                                if pearson_corr < thresholds['pearson_threshold']:\n",
    "                                    suspicion_score += 1\n",
    "                                \n",
    "                                valid_features += 1\n",
    "            \n",
    "            if valid_features > 0:\n",
    "                suspicion_ratio = suspicion_score / (valid_features * 2)\n",
    "                prob_malicious = min(max(suspicion_ratio, 0), 1)  # Clamp between 0 and 1\n",
    "            else:\n",
    "                prob_malicious = 0.5  # Neutral if no valid features\n",
    "            \n",
    "            probabilities.append([1 - prob_malicious, prob_malicious])\n",
    "        \n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40b98c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the complete Benford's Law malware detection process...\n",
      "\n",
      "============================================================\n",
      "TRAINING AND EVALUATING BENFORD'S LAW MODEL\n",
      "============================================================\n",
      "Using 64 features for training\n",
      "Dataset size: 58596 samples\n",
      "Label distribution: [29298 29298]\n",
      "Train set: 46876 samples\n",
      "Test set: 11720 samples\n",
      "Training on 3 selected features...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting the complete Benford\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Law malware detection process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Run the training and evaluation\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m model, results, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_benford_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 68\u001b[0m, in \u001b[0;36mtrain_and_evaluate_benford_model\u001b[0;34m(df, selected_features, label_col)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m BenfordMalwareDetector()\n\u001b[0;32m---> 68\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMaking predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m, in \u001b[0;36mBenfordMalwareDetector.fit\u001b[0;34m(self, X, y, selected_features)\u001b[0m\n\u001b[1;32m     27\u001b[0m benign_data \u001b[38;5;241m=\u001b[39m X[benign_mask][feature]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(benign_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Calculate observed frequencies for benign data\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     observed_freq \u001b[38;5;241m=\u001b[39m \u001b[43mget_digit_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbenign_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Calculate distance metrics\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     kl_div \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_func\u001b[38;5;241m.\u001b[39mkullback_leibler_divergence(observed_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenford_expected)\n",
      "Cell \u001b[0;32mIn[21], line 38\u001b[0m, in \u001b[0;36mget_digit_frequencies\u001b[0;34m(data_series)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mCalculate the frequency of each digit (1-9) in a data series\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Extract first digits\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m first_digits \u001b[38;5;241m=\u001b[39m [\u001b[43mextract_first_digit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m data_series]\n\u001b[1;32m     39\u001b[0m first_digits \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m first_digits \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Remove zeros\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Count frequencies\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 20\u001b[0m, in \u001b[0;36mextract_first_digit\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Take absolute value (modulus operation from paper)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m abs_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Collapse the number to extract first digit\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# D_collapsed = |10 * a / 10^int(log10(a))|\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m abs_value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# For numbers < 1, multiply until >= 1\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for abs(): 'str'"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the model\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives (TN):  {cm[0,0]}\")\n",
    "    print(f\"False Positives (FP): {cm[0,1]}\")\n",
    "    print(f\"False Negatives (FN): {cm[1,0]}\")\n",
    "    print(f\"True Positives (TP):  {cm[1,1]}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def train_and_evaluate_benford_model(df, selected_features, label_col='binary_label'):\n",
    "    \"\"\"\n",
    "    Train and evaluate the Benford's Law malware detection model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING AND EVALUATING BENFORD'S LAW MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = df[selected_features + [col for col in df.columns if col.startswith(('pslist', 'handles', 'malfind', 'psxview', 'dlllist', 'ldrmodules', 'modules', 'svcscan', 'callbacks'))]].copy()\n",
    "    y = df[label_col].values\n",
    "    \n",
    "    # Remove any remaining non-numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "    \n",
    "    print(f\"Using {len(X.columns)} features for training\")\n",
    "    print(f\"Dataset size: {len(X)} samples\")\n",
    "    print(f\"Label distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    # Split data (80-20 split like in the paper)\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    indices = np.random.permutation(len(X))\n",
    "    \n",
    "    train_idx = indices[:split_idx]\n",
    "    test_idx = indices[split_idx:]\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    print(f\"Train set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = BenfordMalwareDetector()\n",
    "    model.fit(X_train, y_train, selected_features)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    train_results = evaluate_model(y_train, y_pred_train, \"Training Set\")\n",
    "    \n",
    "    # Evaluate on test set  \n",
    "    test_results = evaluate_model(y_test, y_pred_test, \"Test Set\")\n",
    "    \n",
    "    # Compare with paper results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON WITH PAPER RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Paper Results (KL + Pearson, α=0.1):\")\n",
    "    print(\"Accuracy: 85.60%, Precision: 88.30%, Recall: 82.08%, F1-Score: 85.08%\")\n",
    "    print(f\"\\nOur Results:\")\n",
    "    print(f\"Accuracy: {test_results['accuracy']:.2%}, Precision: {test_results['precision']:.2%}, \"\n",
    "          f\"Recall: {test_results['recall']:.2%}, F1-Score: {test_results['f1_score']:.2%}\")\n",
    "    \n",
    "    return model, test_results, (X_test, y_test, y_pred_test)\n",
    "\n",
    "# Now let's run everything\n",
    "print(\"Starting the complete Benford's Law malware detection process...\")\n",
    "\n",
    "# Run the training and evaluation\n",
    "model, results, test_data = train_and_evaluate_benford_model(df, selected_features, label_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
